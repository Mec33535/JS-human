<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <title>Human</title>
  <meta name="viewport" content="width=device-width" id="viewport">
  <meta name="keywords" content="Human">
  <meta name="description" content=" ">
  <link rel="manifest" href="../manifest.webmanifest">
  <link rel="shortcut icon" href="" type="image/x-icon">
  <style>
    @font-face {
      font-family: 'Lato';
      font-display: swap;
      font-style: normal;
      font-weight: 100;
      src: local('Lato'), url('../lato-light.woff2')
    }

    body {
      font-family: 'Lato', 'Segoe UI';
      font-size: 16px;
      font-variant: small-caps;
      margin: 0;
      background: black;
      color: white;
      overflow: hidden;
      width: 100vw;
      height: 100vh;
      overflow-x: auto;
    }
  </style>
</head>

<body>
  <canvas id="canvas" style="margin: 0 auto; width: 100vw"></canvas>
  <pre id="log" style="padding: 8px; position: fixed; bottom: 0"></pre>
  <script type="module">
    import * as H from '../../dist/human.esm.js'; // equivalent of import @vladmandic/Human

    const humanConfig = { // user configuration for human, used to fine-tune behavior
      modelBasePath: '../../models', // models can be loaded directly from cdn as well
      filter: { enabled: true, equalization: true, flip: false },
      face: { enabled: true, liveness: { enabled: true }, detector: { rotation: false }, mesh: { enabled: true }, attention: { enabled: false }, iris: { enabled: true }, description: { enabled: true }, emotion: { enabled: true } },
      body: { enabled: false },
      hand: { enabled: true, maxDetected: '1' },
      gesture: { enabled: false },
      object: { enabled: false },
      segmentation: { enabled: false },

    };
    const human = new H.Human(humanConfig); // create instance of human with overrides from user configuration
    const canvas = document.getElementById('canvas'); // output canvas to draw both webcam and detection results

    async function drawLoop() { // main screen refresh loop
      const interpolated = human.next(); // get smoothened result using last-known results which are continously updated based on input webcam video
      human.draw.canvas(human.webcam.element, canvas); // draw webcam video to screen canvas // better than using procesed image as this loop happens faster than processing loop
      const drawoptions = {
        drawGaze: false,
        //useCurves: true,
        drawBoxes: true,
        alpha: 0.5,
        faceLabels: `face
      confidence: [score]%
      [gender] [genderScore]%
      age: [age] years
      distance: [distance]cm
      live: [live]%
      [emotions]
      roll: [roll]째 yaw:[yaw]째 pitch:[pitch]째
      gaze: [gaze]째`,
      };
      await human.draw.all(canvas, interpolated, drawoptions); // draw labels, boxes, lines, etc.
      setTimeout(drawLoop, 20); // use to slow down refresh from max refresh rate to target of 1000/30 ~ 30 fps if it's 30
    }

    async function main() { // main entry point
      //document.getElementById('log').innerHTML = `human version: ${human.version} | tfjs version: ${human.tf.version['tfjs-core']}<br>platform: ${human.env.platform} | agent ${human.env.agent}`;
      await human.webcam.start({ crop: false }); // find webcam and start it
      human.video(human.webcam.element); // instruct human to continously detect video frames
      canvas.width = human.webcam.width; // set canvas resolution to input webcam native resolution
      canvas.height = human.webcam.height;
      canvas.onclick = async () => { // pause when clicked on screen and resume on next click
        if (human.webcam.paused) await human.webcam.play();
        else human.webcam.pause();
      };
      await drawLoop(); // start draw loop
    }

    window.onload = main;
    console.log("fps: " + 1000 / 20);

  </script>
</body>

</html>